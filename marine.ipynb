{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesús Matos Torres, Ricardo Cárdenes Pérez, Susana Suárez Mendoza, Jia Hao Yang, Óscar Rico Rodríguez y Aurora Zouris\n",
    "\n",
    "import IPython\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from torchlibrosa.stft import Spectrogram\n",
    "import librosa\n",
    "import librosa.display \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from scipy.io import wavfile as wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio_fragment(filename, start, end, samplerate = 50000):\n",
    "    \"\"\"Play a fragment of an audio file.\n",
    "    Args:\n",
    "        filename: path to the audio file\n",
    "        start: start of the fragment in samples\n",
    "        end: end of the fragment in samples\n",
    "        samplerate: samplerate to use when reading the file\"\"\"\n",
    "    if not filename.startswith(\".\"):\n",
    "      filename = f\"https://storage.googleapis.com/datathon2022/dataset1/{filename}.ogg\"\n",
    "\n",
    "    if filename.startswith(\"http\"):\n",
    "        filename = io.BytesIO(urlopen(filename).read())\n",
    "\n",
    "    data, read_sr = sf.read(filename, start=start, stop=end)\n",
    "    assert samplerate == read_sr, f\"samplerate does not match {samplerate} (from file) != {read_sr} (function parameter)\"\n",
    "\n",
    "    IPython.display.display(IPython.display.Audio(data, rate=samplerate))\n",
    "\n",
    "def play_annotation_from_df(row, margin: int = 0, samplerate = 50000):\n",
    "    \"\"\"Play a fragment of a wav file in a jupyter notebook.\n",
    "    Args:\n",
    "        row: a row of a pandas dataframe with the following columns:\n",
    "            - path: path to the wav file\n",
    "            - offset: offset in seconds\n",
    "            - duration: duration in seconds\n",
    "        margin: margin in seconds to add to the start and end of the fragment\n",
    "        samplerate: samplerate to use when reading the file\n",
    "        \"\"\"\n",
    "    m = margin * samplerate # margin in samples\n",
    "    start = max(int(np.floor(row['start'] - m)), 0)\n",
    "    end = int(np.ceil(row['start'] + row['duration'] * samplerate + m))\n",
    "    filename = row['path']\n",
    "    print(row)\n",
    "    play_audio_fragment(filename, start, end, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mhttps://bit.ly/dataset1marine\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset1, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m      3\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mdataset1 is not a DataFrame\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv(\"https://bit.ly/dataset1marine\")\n",
    "if not isinstance(dataset1, pd.DataFrame):\n",
    "  assert False, \"dataset1 is not a DataFrame\"\n",
    "\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_extractor = Spectrogram(\n",
    "    win_length=1024, \n",
    "    hop_length=320\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw audio data\n",
    "example, _ = librosa.load('input/first.wav', sr=32000, mono=True)\n",
    "raw_audio = torch.Tensor(example).unsqueeze(0).cuda()\n",
    "spectrogram = spectrogram_extractor(raw_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 20))\n",
    "librosa.display.specshow(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to input and output data\n",
    "INPUT_DIR = '/content/input'\n",
    "OUTPUT_DIR = '/content/output'\n",
    "\n",
    "# Print names of 10 WAV files from the input path\n",
    "parent_list = os.listdir(INPUT_DIR)\n",
    "for i in range(len(parent_list)):\n",
    "    print(parent_list[i])\n",
    "\n",
    "signal_wave = spectrogram\n",
    "sample_rate = 16000\n",
    "sig = np.frombuffer(signal_wave.readframes(sample_rate), dtype=np.int16)\n",
    "plt.figure(figsize=(12,12))\n",
    "plot_a = plt.subplot(211)\n",
    "plot_a.set_title(parent_list[i])\n",
    "plot_a.plot(sig)\n",
    "plot_a.set_xlabel('sample rate * time')\n",
    "plot_a.set_ylabel('energy')\n",
    "\n",
    "plot_b = plt.subplot(212)\n",
    "plot_b.specgram(sig, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "plot_b.set_xlabel('Time')\n",
    "plot_b.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, wave_audio = wav.read('input/first.wav')\n",
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_rate = librosa.load('input/second.ogg')\n",
    "plt.figure(figsize=(500,50))\n",
    "librosa.display.waveshow(data, sr = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1): \n",
    "    signal_wave = data\n",
    "    sample_rate = 16000\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_a = plt.subplot(211)\n",
    "    plot_a.set_title(parent_list[i])\n",
    "    plot_a.plot(data)\n",
    "    plot_a.set_xlabel('sample rate * time')\n",
    "    plot_a.set_ylabel('energy')\n",
    "\n",
    "    plot_b = plt.subplot(212)\n",
    "    plot_b.specgram(data, NFFT=1024, Fs=sample_rate, noverlap=15000)\n",
    "    plot_b.set_xlabel('Time')\n",
    "    plot_b.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_rate = librosa.load('/content/input/a5f4867089fe98f346a7759237f78323.ogg')\n",
    "plt.figure(figsize=(500,50))\n",
    "librosa.display.waveshow(data, sr = sample_rate)\n",
    "\n",
    "for i in range(1): \n",
    "    \n",
    "    sample_rate = 16000\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_a = plt.subplot(211)\n",
    "    plot_a.set_title(parent_list[i])\n",
    "    plot_a.plot(data)\n",
    "    plot_a.set_xlabel('sample rate * time')\n",
    "    plot_a.set_ylabel('energy')\n",
    "\n",
    "    plot_b = plt.subplot(212)\n",
    "    plot_b.specgram(data, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "    plot_b.set_xlabel('Time')\n",
    "    plot_b.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_list = os.listdir(INPUT_DIR)\n",
    "pf = []\n",
    "for i in range(len(parent_list)):\n",
    "  if (parent_list[i][-3] == 'o' or parent_list[i][-3] == 'w'):\n",
    "    pf.append(parent_list[i])\n",
    "\n",
    "parent_list = pf\n",
    "\n",
    "print(parent_list)\n",
    "\n",
    "for i in range(len(parent_list)): \n",
    "    \n",
    "    sample_rate = 16000\n",
    "\n",
    "    print(f'input/{parent_list[i]}')\n",
    "    data, sample_rate = librosa.load(f'input/{parent_list[i]}')\n",
    "    print(data)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_b = plt.subplot(212)\n",
    "    plot_b.specgram(data, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "    plot_b.set_xlabel('Time')\n",
    "    plot_b.set_ylabel('Frequency')\n",
    "\n",
    "    plt.savefig(f'images2/example{i}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get sound and frame rate info\n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "# For every recording, make a spectogram and save it as label_speaker_no.png\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "    \n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[0]}'\n",
    "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            pylab.specgram(sound_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()\n",
    "\n",
    "# Print the ten classes in our dataset\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "print(\"Classes: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])\n",
    "    \n",
    "# File names for class 1\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1'))\n",
    "print(\"\\nA few example files: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = []\n",
    "parent_list = os.listdir(os.path.join('/content/', 'images'))\n",
    "\n",
    "for i in range(len(parent_list)):\n",
    "  if (parent_list[i][0] != '.'):\n",
    "    pf.append(parent_list[i])\n",
    "\n",
    "parent_list = pf\n",
    "print(parent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 2\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.listdir(os.path.join('/content/images/', 'class_0')),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.listdir(os.path.join('/content/images', 'images')),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bd35b774331e1e0d752e2dfbb11db0a7a5bc150e8fc3d8e7f167268d30a1b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
